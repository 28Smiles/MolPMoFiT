{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General-Domain MSPM Training\n",
    "\n",
    "In this notebook, we are going to train an universal molecular structure prediction model (MSPM) on **one million** compounds curated from [ChEMBL](https://www.ebi.ac.uk/chembl/). \n",
    "\n",
    "We use [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) as molecuar representation. SMILES is a type of textual represetnation for molecules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [12:45:09] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from utils import *\n",
    "\n",
    "torch.cuda.set_device(1) #change to 0 if you only has one GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the datasets. The train and valid datasets were splited randomly with a ratio of 0.98:0.02. The training set is large and was stored as a zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((980000, 2), (20000, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/MSPM/ChemBL-LM_train.zip', compression='zip')\n",
    "valid = pd.read_csv('../data/MSPM/ChemBL-LM_val.csv')\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each molecular structure can be represented by multiple SMIMES. [Josep Ar√∫s-Pous et.al.](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-019-0393-0) found that randomized SMILES strings imporve the quality of molecular generative models. We randomized the SMILES to augment the dataset. The `Randomized SMILES` are generated by randomizing the the atom IDs of a molecular graph (see `randomize_smiles()` in utils.py).\n",
    "\n",
    "We generated **4** Randomized SMILES for each molecule. It will take a while to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_aug = smiles_augmentation(train,4)\n",
    "valid_aug = smiles_augmentation(valid,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a path to save the resluts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path('../results')\n",
    "name = 'MSPM'\n",
    "path = result_path/name\n",
    "path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "mdl_path = path/'models'\n",
    "mdl_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to tokenize SMILES and map each token into a unique token ID. (Detials see `MolTokenizer()` in the utils.py and [`Tokenizer()`](https://docs.fast.ai/text.transform.html#Tokenizer) in fastai library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(partial(MolTokenizer, special_tokens = special_tokens), n_cpus=6, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [DataBunch](https://docs.fast.ai/text.data.html#TextLMDataBunch) for training the model. This will take a relative long time. We only need to run it once and load the saved files later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bs = 128 # batch size\n",
    "\n",
    "data = TextLMDataBunch.from_df(path, train_aug, valid_aug, bs=bs, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols=0, max_vocab=60000, include_bos=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One batch of the data. [BOS] is prepended to the SMILES, means the start of the SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>) F ) n n c 1 N 1 C [C@H] ( c 2 s c ( Cl ) c c 2 ) N ( C ( C 2 C C S ( = O ) ( = O ) C C 2 ) = O ) C C 1 [BOS] c 1 2 [nH] c n c 1 c c ( C ( N 1 [C@H] 3 C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>C ( N c 1 n [nH] c 2 n c ( - c 3 c c c o 3 ) c ( Br ) c c 1 2 ) C 1 C C N ( C c 2 c c c c c 2 ) C 1 [BOS] C 1 C 2 ( O O C 3 ( O O 2 ) C C C C C C C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>O ) n ( C ) c 3 N C 3 = C 2 C ( = O ) c 2 c 3 c c c c 2 ) c c ( C ( F ) ( F ) F ) c 1 ) ( F ) ( F ) F [BOS] N ( C ( = O ) c 1 n ( - c 2 c ( Cl )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>C ) c c 3 C ) C C 2 ) = O ) c c 1 [BOS] C c 1 c ( Cl ) c c c c 1 N C ( = O ) C C C ( = O ) N / N = C / c 1 c c c [nH] 1 [BOS] C C C 1 C c 2 c ( C ) n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>( n 2 c c ( C ( N ) = O ) c ( N c 3 c c n c ( F ) c 3 ) n 2 ) C C N ( C ( = O ) O C C ( F ) F ) C [C@@H] 1 F [BOS] s 1 c ( N C ( = O ) C S C c 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the databunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save(f'{name}_databunch')\n",
    "len(data.vocab.itos),len(data.train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traain the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the databunch generated in last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128 # batch size\n",
    "data_lm = load_data(path, f'{name}_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the [model](https://docs.fast.ai/text.learner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = language_model_learner(data_lm, AWD_LSTM, drop_mult=1, pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(64, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(64, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=64, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to train the model. I trained the model on a single **Quadro P4000** GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.785495</td>\n",
       "      <td>0.745334</td>\n",
       "      <td>0.736138</td>\n",
       "      <td>2:23:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.798158</td>\n",
       "      <td>0.755145</td>\n",
       "      <td>0.732615</td>\n",
       "      <td>2:23:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.780080</td>\n",
       "      <td>0.738548</td>\n",
       "      <td>0.737841</td>\n",
       "      <td>2:22:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.760190</td>\n",
       "      <td>0.722149</td>\n",
       "      <td>0.743182</td>\n",
       "      <td>2:22:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.747407</td>\n",
       "      <td>0.706360</td>\n",
       "      <td>0.748099</td>\n",
       "      <td>2:27:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.727602</td>\n",
       "      <td>0.689494</td>\n",
       "      <td>0.753515</td>\n",
       "      <td>2:28:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.712938</td>\n",
       "      <td>0.673928</td>\n",
       "      <td>0.758721</td>\n",
       "      <td>2:28:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.689546</td>\n",
       "      <td>0.658265</td>\n",
       "      <td>0.763684</td>\n",
       "      <td>2:29:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.679796</td>\n",
       "      <td>0.647731</td>\n",
       "      <td>0.767208</td>\n",
       "      <td>2:25:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.675316</td>\n",
       "      <td>0.644556</td>\n",
       "      <td>0.768328</td>\n",
       "      <td>2:25:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 3e-3\n",
    "lr *= bs/48  # Scale learning rate by batch size\n",
    "\n",
    "learner.unfreeze()\n",
    "learner.fit_one_cycle(10, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save both the weights and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fns = [f'{name}_wt', f'{name}_vocab']\n",
    "\n",
    "learner.save(lm_fns[0], with_opt=False)\n",
    "learner.data.vocab.save(mdl_path/(lm_fns[1] + '.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
